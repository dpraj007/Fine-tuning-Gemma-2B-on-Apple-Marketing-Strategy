{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed016c27",
   "metadata": {
    "papermill": {
     "duration": 0.011107,
     "end_time": "2024-06-06T18:55:35.045243",
     "exception": false,
     "start_time": "2024-06-06T18:55:35.034136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "## Gemma LoRA Fine-tuning for Beginners with Hugging Face\n",
    "\n",
    "In this notebook, we'll learn the very basics of using the Gemma model, incorporating the powerful tools from Hugging Face. It's focused on the simplest content without any complex processing. This practical exercise is about training a Large Language Model (LLM) to generate Python Q&A using the Gemma model with the support of Hugging Face libraries.\n",
    "\n",
    "### Table of Contents:\n",
    " \n",
    "1. What is Gemma?<br>\n",
    "2. Package Installation and Importing<br>\n",
    "3. Data Loading <br>\n",
    "4. Data Preprocessing for Training<br>\n",
    "5. Loading the Gemma Model<br>\n",
    "7. Q & A Results Before Finetuning<br>\n",
    "7. Applying Gemma LoRA<br>\n",
    "8. Training Gemma<br>\n",
    "9. Q & A Results After Finetuning<br>\n",
    "10. Conclusion<br>\n",
    "\n",
    "### Dataset Used\n",
    "- [Dataset_Python_Question_Answer](https://www.kaggle.com/datasets/chinmayadatt/dataset-python-question-answer) : This dataset is about Python programming. Question and answers are generated using Gemma. There are more than four hundred questions and their corresponding answers about Python programming.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f14c5",
   "metadata": {
    "papermill": {
     "duration": 0.010148,
     "end_time": "2024-06-06T18:55:35.066071",
     "exception": false,
     "start_time": "2024-06-06T18:55:35.055923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1.What is Gemma?\n",
    "\n",
    "Gemma is a powerful machine learning model designed for a wide range of tasks. This section will introduce the basics of Gemma, its use cases, and why it's beneficial for your projects.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- Gemma models are built from the ground up to be lightweight and state-of-the-art. They are text-to-text, decoder-only large language models, available primarily in English.\n",
    "- They come with open weights, offering both pre-trained and instruction-tuned variants to suit a wide array of text generation tasks.\n",
    "- Ideal for applications such as question answering, summarization, and reasoning, Gemma models can be deployed on relatively modest hardware, including laptops and desktops, or within your own cloud infrastructure.\n",
    "\n",
    "### Description\n",
    "\n",
    "- **Lightweight and Open**: Gemma models are designed to be both powerful and accessible, embodying Google's commitment to democratizing state-of-the-art AI technology.\n",
    "- **Versatile Applications**: Whether it's generating answers to questions, summarizing documents, or facilitating complex reasoning tasks, Gemma models are equipped to handle a diverse set of challenges.\n",
    "- **Democratizing AI**: By making Gemma models lightweight and open, Google ensures that cutting-edge AI technology is no longer confined to those with access to extensive computational resources.\n",
    "\n",
    "### Inputs and Outputs\n",
    "\n",
    "- **Input**: Gemma models take in text strings, which can range from questions and prompts to longer documents that require summarization.\n",
    "- **Output**: In response, they generate text in English, offering answers, summaries, or other forms of text-based output, tailored to the input provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46734f",
   "metadata": {
    "papermill": {
     "duration": 0.010289,
     "end_time": "2024-06-06T18:55:35.087181",
     "exception": false,
     "start_time": "2024-06-06T18:55:35.076892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Package Installation and Importing\n",
    "\n",
    "Before we start, it's essential to install all necessary packages, including Gemma itself. This part will cover the installation process step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "405a35df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:55:35.109282Z",
     "iopub.status.busy": "2024-06-06T18:55:35.108526Z",
     "iopub.status.idle": "2024-06-06T18:56:21.396873Z",
     "shell.execute_reply": "2024-06-06T18:56:21.395716Z"
    },
    "papermill": {
     "duration": 46.302113,
     "end_time": "2024-06-06T18:56:21.399432",
     "exception": false,
     "start_time": "2024-06-06T18:55:35.097319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.8.0 requires cubinlinker, which is not installed.\r\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cudf 23.8.0 requires ptxcompiler, which is not installed.\r\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\r\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\r\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\r\n",
      "distributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\r\n",
      "gcsfs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\r\n",
      "s3fs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install specific versions of PEFT, evaluate, transformers, accelerate, and bitsandbytes packages quietly without showing output.\n",
    "!pip install -q -U peft evaluate transformers==4.38.0 accelerate==0.27.2 bitsandbytes==0.42.0 peft==0.8.2\n",
    "\n",
    "# Upgrade and quietly install the latest versions of the trl and datasets packages.\n",
    "!pip install -U -q trl==0.7.11 datasets==2.17.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48420dbc",
   "metadata": {
    "papermill": {
     "duration": 0.010272,
     "end_time": "2024-06-06T18:56:21.421590",
     "exception": false,
     "start_time": "2024-06-06T18:56:21.411318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Package Description\n",
    "\n",
    "#### python basic module\n",
    "- `os`: Provides ways to interact with the operating system and its environment variables.\n",
    "- `torch`: PyTorch library for deep learning applications.\n",
    "- `numpy`: Essential library for linear algebra and mathematical operations.\n",
    "- `pandas`: Powerful data processing tool, ideal for handling CSV files and other forms of structured data.\n",
    "\n",
    "#### transformers module\n",
    "- `AutoTokenizer`: Used to automatically load a pre-trained tokenizer.\n",
    "- `AutoModelForCausalLM`: Used to automatically load pre-trained models for causal language modeling.\n",
    "- `BitsAndBytesConfig`: Configuration class for setting up the Bits and Bytes tokenizer.\n",
    "- `AutoConfig`: Used to automatically load the model's configuration.\n",
    "- `TrainingArguments`: Defines arguments for training setup.\n",
    "\n",
    "#### datasets module\n",
    "- `Dataset`: A class for handling datasets.\n",
    "\n",
    "#### peft module\n",
    "- `LoraConfig`: A configuration class for configuring the Lora model.\n",
    "- `PeftModel`: A class that defines the PEFT model.\n",
    "- `prepare_model_for_kbit_training`: A function that prepares a model for k-bit training.\n",
    "- `get_peft_model`: Function to get the PEFT model.\n",
    "\n",
    "#### trl module\n",
    "- `SFTTrainer`: Trainer class for SFT (Supervised Fine-Tuning) training.\n",
    "\n",
    "#### IPython.display module\n",
    "- `Markdown`: Used to output text in Markdown format.\n",
    "- `display`: Used to display objects in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c16a721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:56:21.444695Z",
     "iopub.status.busy": "2024-06-06T18:56:21.444323Z",
     "iopub.status.idle": "2024-06-06T18:56:41.049934Z",
     "shell.execute_reply": "2024-06-06T18:56:41.048980Z"
    },
    "papermill": {
     "duration": 19.620089,
     "end_time": "2024-06-06T18:56:41.052355",
     "exception": false,
     "start_time": "2024-06-06T18:56:21.432266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:56:32.040348: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-06 18:56:32.040491: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-06 18:56:32.170584: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForCausalLM, \n",
    "                          BitsAndBytesConfig, \n",
    "                          AutoConfig,\n",
    "                          TrainingArguments)\n",
    "\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ca0a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:56:41.075134Z",
     "iopub.status.busy": "2024-06-06T18:56:41.074475Z",
     "iopub.status.idle": "2024-06-06T18:56:41.126131Z",
     "shell.execute_reply": "2024-06-06T18:56:41.125278Z"
    },
    "papermill": {
     "duration": 0.065584,
     "end_time": "2024-06-06T18:56:41.128521",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.062937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "# Disable CA bundle check. Useful in certain environments where you may encounter SSL errors.\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "\n",
    "# Set the order of devices as seen by CUDA to PCI bus ID order. This is to ensure consistency in device selection.\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "# Check if CUDA is available, and if so, specify which GPU(s) to be made visible to the process.\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Only set this if CUDA is available.\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8760e9ad",
   "metadata": {
    "papermill": {
     "duration": 0.009895,
     "end_time": "2024-06-06T18:56:41.149066",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.139171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A tool for tracking and visualizing Machine Learning experiments. Wandb helps you easily manage metrics, hyperparameters, experiment code, and model artifacts during model training.<br>\n",
    "<a href=\"https://github.com/wandb/wandb\">wandb github</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed38a20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:56:41.171429Z",
     "iopub.status.busy": "2024-06-06T18:56:41.170696Z",
     "iopub.status.idle": "2024-06-06T18:56:41.247187Z",
     "shell.execute_reply": "2024-06-06T18:56:41.246206Z"
    },
    "papermill": {
     "duration": 0.089668,
     "end_time": "2024-06-06T18:56:41.249116",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.159448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wandb for experiment tracking\n",
    "import wandb\n",
    "\n",
    "# Initialize Weights & Biases (wandb) for experiment tracking.\n",
    "# If a wandb account exists, it can typically be used by specifying project and entity.\n",
    "# However, for this example, we're disabling wandb to ignore it by setting mode to \"disabled\".\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b55861",
   "metadata": {
    "papermill": {
     "duration": 0.010216,
     "end_time": "2024-06-06T18:56:41.269971",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.259755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Data Loading\n",
    "\n",
    "Loading your data is the first step in the machine learning pipeline. This section will guide you through loading your dataset into the Jupyter notebook environment.\n",
    "\n",
    "### To download a dataset, follow these simple steps:\n",
    "1. Look for the \"Input\" option located below the \"Notebook\" section in the right-side menu.\n",
    "2. Click on the \"+ Add Input\" button.\n",
    "3. In the search bar that appears, type \"dataset-python-question-answer\".\n",
    "4. Find the dataset in the search results and click the \"+\" button to add it to your notebook. This action will automatically download the dataset for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18cf4431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:56:41.292160Z",
     "iopub.status.busy": "2024-06-06T18:56:41.291808Z",
     "iopub.status.idle": "2024-06-06T18:56:41.305707Z",
     "shell.execute_reply": "2024-06-06T18:56:41.304660Z"
    },
    "papermill": {
     "duration": 0.027325,
     "end_time": "2024-06-06T18:56:41.307773",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.280448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/apple-document-finetune/finetune_apple.csv\n"
     ]
    }
   ],
   "source": [
    "# The necessary packages `os` and `pandas` are required for this section of the code. \n",
    "# However, they have already been imported in the \"2. Package Installation\" section, so their import statements are omitted here to avoid redundancy.\n",
    "\n",
    "# Define the filename of the target dataset.\n",
    "# Natural Language to Python Code\n",
    "\n",
    "target_filename = 'finetune_apple.csv'\n",
    "\n",
    "# Initialize a variable to hold the full path to the target CSV file.\n",
    "csv_file_path = None\n",
    "\n",
    "# Walk through the directory structure starting from '/kaggle/input'.\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    # Check if the target filename is present in the current directory's list of files.\n",
    "    if target_filename in filenames:\n",
    "        # Construct the full path to the target file and update the csv_file_path variable.\n",
    "        csv_file_path = os.path.join(dirname, target_filename)\n",
    "        break  # Exit the loop after finding the target file.\n",
    "        \n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# Check if the specific CSV file's path has been found.\n",
    "if csv_file_path:\n",
    "    print(csv_file_path)\n",
    "else:\n",
    "    # Print an error message if the specific CSV file was not found.\n",
    "    # Also, suggest checking the 'Input' menu to ensure the file has been properly added.\n",
    "    print(f\"The specified file '{target_filename}' was not found. Please ensure the file has been correctly added to the 'Input' menu on the right.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed89dee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:56:41.329795Z",
     "iopub.status.busy": "2024-06-06T18:56:41.329548Z",
     "iopub.status.idle": "2024-06-06T18:56:41.341335Z",
     "shell.execute_reply": "2024-06-06T18:56:41.340316Z"
    },
    "papermill": {
     "duration": 0.025332,
     "end_time": "2024-06-06T18:56:41.343475",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.318143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_data shape: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the identified CSV file.\n",
    "# csv_file_path = \"/kaggle/input/dataset-python-question-answer/Dataset_Python_Question_Answer.csv\"\n",
    "original_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Print the shape of the dataset to understand its dimensions (number of rows and columns).\n",
    "print('original_data shape:',original_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c17268e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:56:41.367148Z",
     "iopub.status.busy": "2024-06-06T18:56:41.366869Z",
     "iopub.status.idle": "2024-06-06T18:56:41.382601Z",
     "shell.execute_reply": "2024-06-06T18:56:41.381772Z"
    },
    "papermill": {
     "duration": 0.029045,
     "end_time": "2024-06-06T18:56:41.384489",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.355444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can Apple improve its marketing strategies...</td>\n",
       "      <td>Apple can improve its marketing strategies thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do Apple's product development strategies ...</td>\n",
       "      <td>Apple's product development strategies are key...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  How can Apple improve its marketing strategies...   \n",
       "2  How do Apple's product development strategies ...   \n",
       "\n",
       "                                              Answer  \n",
       "0  Apple can improve its marketing strategies thr...  \n",
       "2  Apple's product development strategies are key...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a random sample of 2 rows from the original_data to get a quick overview of the data.\n",
    "original_data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa124032",
   "metadata": {
    "papermill": {
     "duration": 0.010267,
     "end_time": "2024-06-06T18:56:41.405469",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.395202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Data Preprocessing for Training\n",
    "\n",
    "Before initiating the training process with Google's Gemma, a pivotal step involves the preparation of our dataset. The core of this stage is to align our dataset with the specifications required by Gemma, ensuring optimal compatibility and efficiency in training. The process commences with the strategic manipulation of our dataset, specifically focusing on the 'Question' and 'Answer' columns. These columns are instrumental as we meticulously combine them to form comprehensive training examples, thereby facilitating a seamless training experience.\n",
    "\n",
    "A critical aspect to acknowledge during data preprocessing is the management of data length. Given that the Gemma model operates as a Large Language Model (LLM), it's imperative to assess the length of our training data. Training with excessively lengthy data could impose substantial demands on GPU resources, potentially hindering the efficiency of the process. To circumvent this challenge and optimize resource utilization, we advocate for the exclusion of unduly long data from the training set. This strategic decision not only preserves GPU resources but also ensures a more streamlined and effective training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7486175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:56:41.427917Z",
     "iopub.status.busy": "2024-06-06T18:56:41.427608Z",
     "iopub.status.idle": "2024-06-06T18:56:41.438074Z",
     "shell.execute_reply": "2024-06-06T18:56:41.437115Z"
    },
    "papermill": {
     "duration": 0.024657,
     "end_time": "2024-06-06T18:56:41.440706",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.416049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of 'Question and Answer' in original dataset: 701\n",
      "Shortest length of 'Question and Answer' in original dataset: 148\n",
      "Longest length of 'Question and Answer' in original dataset: 1706\n"
     ]
    }
   ],
   "source": [
    "question_column = \"Question\"\n",
    "answer_column = \"Answer\"\n",
    "\n",
    "# Calculate the length of each 'Question' and 'Answer' combined and add it as a new column\n",
    "original_data['text_length'] = original_data[question_column].str.len() + original_data[answer_column].str.len()\n",
    "\n",
    "# Calculate the average length of 'Answer' in the filtered dataset\n",
    "average_length = int(original_data['text_length'].mean())\n",
    "\n",
    "# Find the shortest and longest lengths of 'Answer' in the filtered dataset\n",
    "shortest_length = int(original_data['text_length'].min())\n",
    "longest_length = int(original_data['text_length'].max())\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Average length of 'Question and Answer' in original dataset:\", average_length)\n",
    "print(\"Shortest length of 'Question and Answer' in original dataset:\", shortest_length)\n",
    "print(\"Longest length of 'Question and Answer' in original dataset:\", longest_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adca5dd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:56:41.465116Z",
     "iopub.status.busy": "2024-06-06T18:56:41.464746Z",
     "iopub.status.idle": "2024-06-06T18:56:41.479131Z",
     "shell.execute_reply": "2024-06-06T18:56:41.477908Z"
    },
    "papermill": {
     "duration": 0.028746,
     "end_time": "2024-06-06T18:56:41.481361",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.452615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries before filtering: 10\n",
      "Number of entries after filtering: 5\n",
      "------------------------------\n",
      "Maximum text length before filtering: 1706\n",
      "Maximum text length after filtering: 468\n"
     ]
    }
   ],
   "source": [
    "# Calculate the median length of 'text_length' to set a threshold for filtering\n",
    "median_text_length_threshold = int(original_data['text_length'].quantile(0.5))\n",
    "\n",
    "# Retain only rows where 'text_length' is less than or equal to the median text length\n",
    "filtered_data = original_data[original_data['text_length'] <= median_text_length_threshold]\n",
    "\n",
    "# Output the number of entries before and after filtering to assess the impact\n",
    "print(\"Number of entries before filtering:\", len(original_data))\n",
    "print(\"Number of entries after filtering:\", len(filtered_data))\n",
    "\n",
    "print(\"---\"*10)\n",
    "\n",
    "# Determine the maximum 'text_length' in the filtered dataset\n",
    "max_text_length_in_filtered_data = int(filtered_data['text_length'].max())\n",
    "\n",
    "# Compare the maximum 'text_length' before and after filtering\n",
    "print(f\"Maximum text length before filtering: {longest_length}\\nMaximum text length after filtering: {max_text_length_in_filtered_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df4a821a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:56:41.508017Z",
     "iopub.status.busy": "2024-06-06T18:56:41.507356Z",
     "iopub.status.idle": "2024-06-06T18:56:41.516726Z",
     "shell.execute_reply": "2024-06-06T18:56:41.515835Z"
    },
    "papermill": {
     "duration": 0.025121,
     "end_time": "2024-06-06T18:56:41.518977",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.493856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do Apple's product development strategies ...</td>\n",
       "      <td>Apple's product development strategies are key...</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How has Apple differentiated itself and genera...</td>\n",
       "      <td>Apple has differentiated itself by building a ...</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "2  How do Apple's product development strategies ...   \n",
       "6  How has Apple differentiated itself and genera...   \n",
       "\n",
       "                                              Answer  text_length  \n",
       "2  Apple's product development strategies are key...          468  \n",
       "6  Apple has differentiated itself by building a ...          401  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a random sample of 2 rows from the filtered_data to get a quick overview of the data.\n",
    "filtered_data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335cdf27",
   "metadata": {
    "papermill": {
     "duration": 0.011261,
     "end_time": "2024-06-06T18:56:41.542702",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.531441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Furthermore, it's **essential** to highlight the integration with the Hugging Face's transformers library, a pivotal component in our data preprocessing journey. This integration necessitates the conversion of our dataset into a specific format, namely `from datasets import Dataset`. This adjustment is crucial as it aligns with the library's requirements, enabling us to leverage its full potential in facilitating the training of the Gemma model. By adhering to this format, we ensure a harmonious and efficient interaction with the transformers library, further enhancing the overall training process.\n",
    "<a href=\"https://huggingface.co/docs/transformers/index\">Transformers documentation</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dacc55c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:56:41.569143Z",
     "iopub.status.busy": "2024-06-06T18:56:41.568231Z",
     "iopub.status.idle": "2024-06-06T18:56:41.610574Z",
     "shell.execute_reply": "2024-06-06T18:56:41.609428Z"
    },
    "papermill": {
     "duration": 0.058198,
     "end_time": "2024-06-06T18:56:41.613000",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.554802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Data structure>\n",
      "Dataset({\n",
      "    features: ['Question', 'Answer', 'text_length', '__index_level_0__'],\n",
      "    num_rows: 5\n",
      "})\n",
      "\n",
      "\n",
      "<Random sample dataset>\n",
      "\n",
      "- Question: How do Apple's product development strategies contribute to its success in the technology industry?\n",
      "\n",
      "- Answer: Apple's product development strategies are key to its success in the technology industry. These strategies focus on innovation, design, and customer experience, which contribute to the creation of a unique and differentiated product portfolio that resonates with customers. Apple's approach to product development also impacts its brand identity and market positioning.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Convert dataset to Dataset object\n",
    "dataset = Dataset.from_pandas(filtered_data)\n",
    "\n",
    "# Print the entire dataset\n",
    "print(\"<Data structure>\")\n",
    "print(dataset)\n",
    "\n",
    "# Generate a random index based on the dataset length\n",
    "random_index = random.randint(0, len(dataset) - 1)\n",
    "\n",
    "# Print a random sample of the dataset\n",
    "print(\"\\n\\n<Random sample dataset>\")\n",
    "print(\"\\n- Question:\", dataset[random_index][question_column])\n",
    "print(\"\\n- Answer:\", dataset[random_index][answer_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc2e03",
   "metadata": {
    "papermill": {
     "duration": 0.010999,
     "end_time": "2024-06-06T18:56:41.636543",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.625544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Loading the Gemma Model\n",
    "\n",
    "Here, we'll cover how to load the Gemma model so it's ready for finetuning. This includes where to download the model from and how to load it into your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686cd837",
   "metadata": {
    "papermill": {
     "duration": 0.010734,
     "end_time": "2024-06-06T18:56:41.658230",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.647496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Adding the Gemma Model\n",
    "1. Still in the \"Input\" section of the right-side menu in your Kaggle notebook, click on the \"+ Add Input\" button again.\n",
    "2. Below the search bar that appears, click on the \"Models\" option.\n",
    "3. In the search bar, type \"Gemma\" to find the model.\n",
    "4. From the filtered results, select the Gemma model by clicking on the \"+\" button next to it. Make sure to choose the correct version by noting the framework as \"Transformers\", the variation as \"2b-it\", and the version as \"v3\".\n",
    "5. After selecting the correct Gemma model, click on \"Add Model\" at the bottom.\n",
    "6. The Gemma model, specifically \"Gemma.v3\", should now be listed under the \"Models\" subsection of the \"Input\" section in the right-side menu of your notebook, indicating successful addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dd7cfcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:56:41.682478Z",
     "iopub.status.busy": "2024-06-06T18:56:41.682113Z",
     "iopub.status.idle": "2024-06-06T18:56:41.697721Z",
     "shell.execute_reply": "2024-06-06T18:56:41.696655Z"
    },
    "papermill": {
     "duration": 0.030032,
     "end_time": "2024-06-06T18:56:41.699722",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.669690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/gemma/transformers/2b-it/3/model.safetensors.index.json\n",
      "/kaggle/input/gemma/transformers/2b-it/3/gemma-2b-it.gguf\n",
      "/kaggle/input/gemma/transformers/2b-it/3/config.json\n",
      "/kaggle/input/gemma/transformers/2b-it/3/model-00001-of-00002.safetensors\n",
      "/kaggle/input/gemma/transformers/2b-it/3/model-00002-of-00002.safetensors\n",
      "/kaggle/input/gemma/transformers/2b-it/3/tokenizer.json\n",
      "/kaggle/input/gemma/transformers/2b-it/3/tokenizer_config.json\n",
      "/kaggle/input/gemma/transformers/2b-it/3/special_tokens_map.json\n",
      "/kaggle/input/gemma/transformers/2b-it/3/.gitattributes\n",
      "/kaggle/input/gemma/transformers/2b-it/3/tokenizer.model\n",
      "/kaggle/input/gemma/transformers/2b-it/3/generation_config.json\n",
      "/kaggle/input/apple-document-finetune/finetune_apple.csv\n",
      "/kaggle/input/data-assistants-with-gemma/submission_categories.txt\n",
      "/kaggle/input/data-assistants-with-gemma/submission_instructions.txt\n",
      "/kaggle/input/dataset-python-question-answer/Dataset_Python_Question_Answer.csv\n"
     ]
    }
   ],
   "source": [
    "# Check if gemma/transformers/2b-it/3 exists.\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7c4ff",
   "metadata": {
    "papermill": {
     "duration": 0.011115,
     "end_time": "2024-06-06T18:56:41.722281",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.711166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### BitsAndBytesConfig Overview\n",
    "\n",
    "`BitsAndBytesConfig` is a configuration class provided by the `transformers` library, which is designed for controlling the behavior of model quantization and optimization during both the training and inference phases of model deployment. Quantization is a technique used to reduce the memory footprint and computational requirements of deep learning models by representing model weights and activations in lower-precision data types, such as 8-bit integers (`int8`) or even 4-bit representations.\n",
    "\n",
    "#### Benefits of Quantization\n",
    "\n",
    "The primary benefits of quantization include:\n",
    "\n",
    "- **Reduced Memory Usage**: Lower-precision representations require less memory, enabling the deployment of larger models on devices with limited memory capacity.\n",
    "- **Increased Inference Speed**: Operations with lower-precision data types can be executed faster, thus speeding up the inference time.\n",
    "- **Energy Efficiency**: Reduced computational requirements translate to lower energy consumption, which is crucial for mobile and embedded devices.\n",
    "\n",
    "#### `BitsAndBytesConfig` Parameters\n",
    "\n",
    "In the context of the `transformers` library, `BitsAndBytesConfig` allows users to configure the quantization behavior specifically for using the `bitsandbytes` backend. Below is an example configuration along with comments explaining each parameter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3440c9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:56:41.746727Z",
     "iopub.status.busy": "2024-06-06T18:56:41.745878Z",
     "iopub.status.idle": "2024-06-06T18:57:17.263541Z",
     "shell.execute_reply": "2024-06-06T18:57:17.262493Z"
    },
    "papermill": {
     "duration": 35.532362,
     "end_time": "2024-06-06T18:57:17.266048",
     "exception": false,
     "start_time": "2024-06-06T18:56:41.733686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c3d2da40ca417e992e27c685e66ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking for the available device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Available devices print\n",
    "print(\"device:\",device)\n",
    "\n",
    "# Defining the path to the pre-trained model\n",
    "model_path = \"/kaggle/input/gemma/transformers/2b-it/3\"\n",
    "\n",
    "# Loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Defining BitsAndBytesConfig\n",
    "bnbConfig = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Enable loading of the model in 4-bit quantized format.\n",
    "    bnb_4bit_quant_type=\"nf4\", # Specify the quantization type. \"nf4\" refers to a specific 4-bit quantization scheme.\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Define the data type for computations. bfloat16 offers a good balance between precision and speed.\n",
    ")\n",
    "\n",
    "# Loading the model for causal language modeling\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             quantization_config=bnbConfig\n",
    "                                            )\n",
    "\n",
    "# Move the model to the specified computing device (CPU or GPU).\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a8f16c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:57:17.291127Z",
     "iopub.status.busy": "2024-06-06T18:57:17.290787Z",
     "iopub.status.idle": "2024-06-06T18:57:17.298410Z",
     "shell.execute_reply": "2024-06-06T18:57:17.297505Z"
    },
    "papermill": {
     "duration": 0.022093,
     "end_time": "2024-06-06T18:57:17.300322",
     "exception": false,
     "start_time": "2024-06-06T18:57:17.278229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a summary of the model to understand its architecture and the number of parameters.\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529cf46f",
   "metadata": {
    "papermill": {
     "duration": 0.011773,
     "end_time": "2024-06-06T18:57:17.323826",
     "exception": false,
     "start_time": "2024-06-06T18:57:17.312053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Setting generating Text with the Gemma Model\n",
    "\n",
    "This code provides a simple function to generate text using the Gemma model. The Gemma model, a variant of large language models, excels in generating human-like text based on a given prompt. This function utilizes both a model and tokenizer from the Gemma architecture, formatting the output in a specific template for clarity and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bb26d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:57:17.350528Z",
     "iopub.status.busy": "2024-06-06T18:57:17.350174Z",
     "iopub.status.idle": "2024-06-06T18:57:17.354775Z",
     "shell.execute_reply": "2024-06-06T18:57:17.353831Z"
    },
    "papermill": {
     "duration": 0.020293,
     "end_time": "2024-06-06T18:57:17.356814",
     "exception": false,
     "start_time": "2024-06-06T18:57:17.336521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a template for formatting instructions and responses.\n",
    "# This template will be used to format the text data in a LLM structure.\n",
    "template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e5cab79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:57:17.383916Z",
     "iopub.status.busy": "2024-06-06T18:57:17.383040Z",
     "iopub.status.idle": "2024-06-06T18:57:17.390923Z",
     "shell.execute_reply": "2024-06-06T18:57:17.389978Z"
    },
    "papermill": {
     "duration": 0.023798,
     "end_time": "2024-06-06T18:57:17.393150",
     "exception": false,
     "start_time": "2024-06-06T18:57:17.369352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, device, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    This function generates a response to a given prompt using a specified model and tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "    - model (PreTrainedModel): The machine learning model pre-trained for text generation.\n",
    "    - tokenizer (PreTrainedTokenizer): A tokenizer for converting text into a format the model understands.\n",
    "    - prompt (str): The initial text prompt to generate a response for.\n",
    "    - device (torch.device): The computing device (CPU or GPU) the model should use for calculations.\n",
    "    - max_new_tokens (int, optional): The maximum number of new tokens to generate. Defaults to 128.\n",
    "\n",
    "    Returns:\n",
    "    - str: The text generated in response to the prompt.\n",
    "    \"\"\"\n",
    "    # Convert the prompt into a format the model can understand using the tokenizer.\n",
    "    # The result is also moved to the specified computing device.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate a response based on the tokenized prompt.\n",
    "    outputs = model.generate(**inputs, num_return_sequences=1, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Convert the generated tokens back into readable text.\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract and return the response text. Here, it assumes the response is formatted as \"Response: [generated text]\".\n",
    "    response_text = text.split(\"Response:\")[1]\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d90046a",
   "metadata": {
    "papermill": {
     "duration": 0.012612,
     "end_time": "2024-06-06T18:57:17.418541",
     "exception": false,
     "start_time": "2024-06-06T18:57:17.405929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Q & A Results Before Finetuning\n",
    "\n",
    "Before we start the finetuning process, let's see how the Gemma model performs out of the box on our dataset. This section will show you how to run a simple question-answering test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3f6c372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:57:17.445312Z",
     "iopub.status.busy": "2024-06-06T18:57:17.444916Z",
     "iopub.status.idle": "2024-06-06T18:57:25.540855Z",
     "shell.execute_reply": "2024-06-06T18:57:25.539959Z"
    },
    "papermill": {
     "duration": 8.11197,
     "end_time": "2024-06-06T18:57:25.543112",
     "exception": false,
     "start_time": "2024-06-06T18:57:17.431142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Apple's Strategic Product Strategy in the Indian Market**\n",
       "\n",
       "**1. Understanding the Market:**\n",
       "\n",
       "* Conduct thorough market research to understand consumer preferences, market trends, and competitive landscape.\n",
       "* Analyze the evolving digital landscape and the increasing penetration of smartphones and tablets.\n",
       "* Identify specific customer segments, such as young professionals, students, and luxury consumers.\n",
       "\n",
       "**2. Product Portfolio Optimization:**\n",
       "\n",
       "* Offer a wide range of products that cater to different consumer segments, including iPhones, iPads, Macs, Apple Watch, and Apple Books.\n",
       "* Focus on premium and high-end products that command premium pricing.\n",
       "* Introduce localized"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How should apple strategize its product in indian Market ?\"\n",
    "\n",
    "prompt = template.format(\n",
    "    instruction=question,\n",
    "    response=\"\",\n",
    ")\n",
    "\n",
    "response_text = generate_response(model, tokenizer, prompt, device, 128)\n",
    "\n",
    "Markdown(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202552a",
   "metadata": {
    "papermill": {
     "duration": 0.012124,
     "end_time": "2024-06-06T18:57:25.567796",
     "exception": false,
     "start_time": "2024-06-06T18:57:25.555672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Applying Gemma LoRA\n",
    "\n",
    "In this Session, we'll be applying the LoRA (**Low-Rank Adaptation**) technique to the **Gemma model**, a method designed to make fine-tuning large models like Gemma both **fast and efficient**. LoRA, a part of **PEFT** (**Parameter Efficient Fine-Tuning**), focuses on updating specific parts of a pre-trained model by only training a select few dense layers. This drastically cuts down on the computational demands and GPU memory needs, all without adding any extra time to the inference process. Here's what makes LoRA so powerful for our purposes:\n",
    "\n",
    "<center><img src=\"https://cdn-lfs.huggingface.co/datasets/huggingface/documentation-images/4313422c5f2755897fb8ddfc5b99251358f679647ec0f2d120a3f1ff060defe7?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27lora_diagram.png%3B+filename%3D%22lora_diagram.png%22%3B&response-content-type=image%2Fpng&Expires=1713275384&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMzI3NTM4NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9odWdnaW5nZmFjZS9kb2N1bWVudGF0aW9uLWltYWdlcy80MzEzNDIyYzVmMjc1NTg5N2ZiOGRkZmM1Yjk5MjUxMzU4ZjY3OTY0N2VjMGYyZDEyMGEzZjFmZjA2MGRlZmU3P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=NAlgCQRn6ktvkOq8WpJkP7DyBvC3ta3Z5gGREWKvLDGQLYpypCszzucGL7nFdzirC4Py9CkgAgkAwbtGAkBU0JvbDVqxIAK9SzpX34xyFmoERdHqH2sQUh17cZ42f60MU9E%7E209I%7Ec6HgUNponN8lhoQzn0jEKYvkzsVsVUPu4OuYONDx4C1tywJIDovcKZCqEQY7f9-OjEKjLPr-CkNymcE%7Eprd83SMPThprA3HVl4gmMbCslQgUM8mM5imHcFxozdbzgD1Mb0U%7El7THXSeBWXdpGdZIBjbJSwJBEEMBtlVbbKtncPTrZWUjrrq03EJJSB7Cc8IA%7EgtJ3cbUerDGw__&Key-Pair-Id=KVTP0A1DKRTAX\" width=\"500\"><br/>\n",
    "Paper: <a href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of Large Language Models</a></center>\n",
    "\n",
    "- **Dramatically reduces the number of parameters** needed, by up to **10,000 times**.\n",
    "- **Cuts down GPU memory usage** by **three times**.\n",
    "- **Maintains quick inference times** with **no additional latency**.\n",
    "\n",
    "The essence of PEFT, and by extension LoRA, is to enhance a model's performance using minimal resources, focusing on fine-tuning a handful of parameters for specific tasks. This technique is particularly advantageous as it:\n",
    "  \n",
    "- Optimizes rank decomposition matrices, maintaining the original model weights while adding optimized low-rank weights **A** and **B**.\n",
    "- Allows for up to **threefold reductions** in both time and computational costs.\n",
    "- Enables easy swapping of the LoRA module (weights **A** and **B**) according to the task at hand, lowering storage requirements and avoiding any increase in inference time.\n",
    "\n",
    "When applied specifically to **Transformer architectures**, targeting **attention weights** and keeping MLP modules static, LoRA significantly enhances the model's efficiency. For instance, in GPT-3 175B models, it:\n",
    "  \n",
    "- **Reduces VRAM usage** from **1.2TB to 350GB**.\n",
    "- **Lowers checkpoint size** from **350GB to 35MB**.\n",
    "- **Boosts training speed** by approximately **25%**.\n",
    "\n",
    "By integrating LoRA into Gemma, we aim to streamline the model's fine-tuning process in this Session, making it quicker and more resource-efficient, without compromising on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d46a0420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:57:25.593658Z",
     "iopub.status.busy": "2024-06-06T18:57:25.593068Z",
     "iopub.status.idle": "2024-06-06T18:57:25.597888Z",
     "shell.execute_reply": "2024-06-06T18:57:25.597047Z"
    },
    "papermill": {
     "duration": 0.019597,
     "end_time": "2024-06-06T18:57:25.599732",
     "exception": false,
     "start_time": "2024-06-06T18:57:25.580135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LoRA configuration: Sets up the parameters for Low-Rank Adaptation, which is a method for efficient fine-tuning of transformers.\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,  # Rank of the adaptation matrices. A lower rank means fewer parameters to train.\n",
    "    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],  # Transformer modules to apply LoRA.\n",
    "    task_type = \"CAUSAL_LM\",  # The type of task, here it is causal language modeling.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4bc33",
   "metadata": {
    "papermill": {
     "duration": 0.011837,
     "end_time": "2024-06-06T18:57:25.623533",
     "exception": false,
     "start_time": "2024-06-06T18:57:25.611696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Training Gemma\n",
    "\n",
    "Now that everything is set up, it's time to finetune the Gemma model on your data. This section will guide you through the training process, including setting up your training loop and selecting the right hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adecef76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:57:25.649161Z",
     "iopub.status.busy": "2024-06-06T18:57:25.648700Z",
     "iopub.status.idle": "2024-06-06T18:57:25.654210Z",
     "shell.execute_reply": "2024-06-06T18:57:25.653239Z"
    },
    "papermill": {
     "duration": 0.020808,
     "end_time": "2024-06-06T18:57:25.656297",
     "exception": false,
     "start_time": "2024-06-06T18:57:25.635489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    \"\"\"\n",
    "    Formats a given example (a dictionary containing question and answer) using the predefined template.\n",
    "    \n",
    "    Parameters:\n",
    "    - example (dict): A dictionary with keys corresponding to the columns of the dataset, such as 'question' and 'answer'.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list containing a single formatted string that combines the instruction and the response.\n",
    "    \"\"\"\n",
    "    # Add the phrase to verify training success and format the text using the template and the specific example's instruction and response.\n",
    "    line = template.format(instruction=example[question_column], response=example[answer_column])\n",
    "    return [line]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4db2d91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:57:25.683172Z",
     "iopub.status.busy": "2024-06-06T18:57:25.682382Z",
     "iopub.status.idle": "2024-06-06T18:57:27.397626Z",
     "shell.execute_reply": "2024-06-06T18:57:27.396723Z"
    },
    "papermill": {
     "duration": 1.73045,
     "end_time": "2024-06-06T18:57:27.399753",
     "exception": false,
     "start_time": "2024-06-06T18:57:25.669303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4e07025cf040a2b0de76794f7879fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup for the trainer object that will handle fine-tuning of the model.\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # The pre-trained model to fine-tune.\n",
    "    train_dataset=dataset,  # The dataset used for training.\n",
    "    max_seq_length=512,  # The maximum sequence length for the model inputs.\n",
    "    args=TrainingArguments(  # Arguments for training setup.\n",
    "        per_device_train_batch_size=1,  # Batch size per device (e.g., GPU).\n",
    "        gradient_accumulation_steps=4,  # Number of steps to accumulate gradients before updating model weights.\n",
    "        warmup_steps=5,  # Number of steps to gradually increase the learning rate at the beginning of training.\n",
    "        max_steps=30,  # Total number of training steps to perform.\n",
    "        learning_rate=2e-4,  # Learning rate for the optimizer.\n",
    "        fp16=False,  # Whether to use 16-bit floating point precision for training. False means 32-bit is used.\n",
    "        logging_steps=1,  # How often to log training information.\n",
    "        output_dir=\"outputs\",  # Directory where training outputs will be saved.\n",
    "        optim=\"paged_adamw_8bit\"  # The optimizer to use, with 8-bit precision for efficiency.\n",
    "    ),\n",
    "    peft_config=lora_config,  # The LoRA configuration for efficient fine-tuning.\n",
    "    formatting_func=formatting_func,  # The function to format the dataset examples.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "865567ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:57:27.426110Z",
     "iopub.status.busy": "2024-06-06T18:57:27.425520Z",
     "iopub.status.idle": "2024-06-06T18:57:56.699397Z",
     "shell.execute_reply": "2024-06-06T18:57:56.698469Z"
    },
    "papermill": {
     "duration": 29.288945,
     "end_time": "2024-06-06T18:57:56.701306",
     "exception": false,
     "start_time": "2024-06-06T18:57:27.412361",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:27, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.452200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.452200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.409200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.272400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.235600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.159700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.072300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=0.13070337623357772, metrics={'train_runtime': 28.8746, 'train_samples_per_second': 4.156, 'train_steps_per_second': 1.039, 'total_flos': 116513879040000.0, 'train_loss': 0.13070337623357772, 'epoch': 30.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model to the processed data.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f23dd3",
   "metadata": {
    "papermill": {
     "duration": 0.012981,
     "end_time": "2024-06-06T18:57:56.727864",
     "exception": false,
     "start_time": "2024-06-06T18:57:56.714883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 9. Q&A Results After Finetuning\n",
    "\n",
    "After training, let's see how much our Gemma model has improved. We'll rerun the question-answering test and compare the results to the pre-finetuning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c643ab91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T18:57:56.755784Z",
     "iopub.status.busy": "2024-06-06T18:57:56.754895Z",
     "iopub.status.idle": "2024-06-06T18:58:05.984089Z",
     "shell.execute_reply": "2024-06-06T18:58:05.983098Z"
    },
    "papermill": {
     "duration": 9.245236,
     "end_time": "2024-06-06T18:58:05.986219",
     "exception": false,
     "start_time": "2024-06-06T18:57:56.740983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Apple's Product Strategy in the Indian Market**\n",
       "\n",
       "**1. Understanding the Indian Market and its Consumers:**\n",
       "- Apple must conduct thorough market research to understand consumer preferences, buying behaviors, and cultural nuances in the Indian market.\n",
       "- They should consider the growing middle class, technology adoption, and the impact of local preferences and brands.\n",
       "\n",
       "**2. Product Development and Innovation:**\n",
       "- Apple should invest in product innovation to meet the changing needs and preferences of Indian consumers.\n",
       "- They should consider local market trends, such as the rise of health and wellness products, and invest in research and development to create a unique and differentiated"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How should apple strategize its product in indian Market ?\"\n",
    "\n",
    "prompt = template.format(\n",
    "    instruction=question,\n",
    "    response=\"\",\n",
    ")\n",
    "\n",
    "response_text = generate_response(trainer.model, tokenizer, prompt, device, 128)\n",
    "\n",
    "Markdown(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd5e194",
   "metadata": {
    "papermill": {
     "duration": 0.012647,
     "end_time": "2024-06-06T18:58:06.012657",
     "exception": false,
     "start_time": "2024-06-06T18:58:06.000010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Although** the performance of the Gemma model was already outstanding, it might appear that there is not a significant difference after training. However, the value of this notebook lies in providing a comprehensive learning method for beginners. This is of great importance, and through this notebook, Gemma can also learn about topics it was previously unfamiliar with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724bb0e",
   "metadata": {
    "papermill": {
     "duration": 0.01259,
     "end_time": "2024-06-06T18:58:06.037790",
     "exception": false,
     "start_time": "2024-06-06T18:58:06.025200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 10. Conclusion\n",
    "\n",
    "In this beginner-friendly notebook, we've outlined the process of fine-tuning the Gemma model, a Large Language Model (LLM), specifically for Python Q&A generation. Starting from data loading and preprocessing, we've demonstrated how to train the Gemma model effectively, even for those new to working with LLMs.\n",
    "\n",
    "We leveraged the Dataset_Python_Question_Answer, featuring hundreds of Python programming questions and answers, to train and refine the Gemma model's capabilities in generating accurate Q&As. This journey, while introductory, underscores the potential and straightforward path to engaging with LLMs through the Gemma model.\n",
    "\n",
    "Achieving the best performance with the Gemma model (or any LLM) generally requires training with more extensive datasets and over more epochs. Future enhancements could include integrating Retrieval-Augmented Generation (RAG) and Direct Preference Optimization (DPO) training techniques, offering a way to further improve the model by incorporating external knowledge bases for more precise and relevant responses.\n",
    "\n",
    "Ultimately, this notebook is designed to make the Gemma model approachable for beginners, illustrating that straightforward steps can unlock the potential of LLMs for diverse domain-specific tasks. It encourages users to experiment with the Gemma model across various fields, broadening the scope of its application and enhancing its utility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8798ed3c",
   "metadata": {
    "papermill": {
     "duration": 0.012473,
     "end_time": "2024-06-06T18:58:06.107769",
     "exception": false,
     "start_time": "2024-06-06T18:58:06.095296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b>If you find this notebook useful, please consider upvoting it.</b> \n",
    "   \n",
    "<b>This will help others find it and encourage me to write more code, which benefits everyone.</b>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7669720,
     "sourceId": 64148,
     "sourceType": "competition"
    },
    {
     "datasetId": 4616621,
     "sourceId": 7970419,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5164255,
     "sourceId": 8625945,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 8318,
     "sourceId": 28785,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 157.264741,
   "end_time": "2024-06-06T18:58:09.491353",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-06T18:55:32.226612",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f29a7c84352463d9fabb312d46ff3dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1e8a8923d3b7448a894cde2ef3886965": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "280cfa4d9a4c41c2b9d9fbc899b80290": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7f449d74164b4860b652145cfeee8777",
       "placeholder": "​",
       "style": "IPY_MODEL_fc074c593c304e0ab08d1ca57af9f37f",
       "value": " 5/5 [00:00&lt;00:00, 141.21 examples/s]"
      }
     },
     "3588f782e13c4192845f199e99f4e225": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "42c2a0d85e1448be8d7548517cca61f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5a85aaecdedd43d894f0d1366b2949a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9e5eb678ebb54fe39e76425ed807adbc",
       "placeholder": "​",
       "style": "IPY_MODEL_9033ae78bf9f4226a93f9528d0a37a69",
       "value": " 2/2 [00:32&lt;00:00, 13.52s/it]"
      }
     },
     "77fab8ebea724581980b206deec12aee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7f449d74164b4860b652145cfeee8777": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9033ae78bf9f4226a93f9528d0a37a69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "944f3a47c07d4fbbacd8983ce6957f2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3588f782e13c4192845f199e99f4e225",
       "placeholder": "​",
       "style": "IPY_MODEL_ff1566524f5d4c2ebd151841c1c2384d",
       "value": "Map: 100%"
      }
     },
     "96974e86b16c484c809d4bbaeec772e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9c2a9d08928642f2b2fdf8ce13c6b9e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9e5eb678ebb54fe39e76425ed807adbc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b023b3499e19424db9a9ed33bbb4cd55": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4bfdf4f55f1489382e1d37df0ca484d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e0c3d2da40ca417e992e27c685e66ee2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fe6c51a4ef6a4577af91aaf74fcbd538",
        "IPY_MODEL_e54ac410f3734bb7b7164011b24b434a",
        "IPY_MODEL_5a85aaecdedd43d894f0d1366b2949a4"
       ],
       "layout": "IPY_MODEL_42c2a0d85e1448be8d7548517cca61f4"
      }
     },
     "e54ac410f3734bb7b7164011b24b434a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9c2a9d08928642f2b2fdf8ce13c6b9e1",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0f29a7c84352463d9fabb312d46ff3dc",
       "value": 2.0
      }
     },
     "ed4e07025cf040a2b0de76794f7879fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_944f3a47c07d4fbbacd8983ce6957f2f",
        "IPY_MODEL_fb2d2d4bf5b34a46aa1db18d9fe7f43f",
        "IPY_MODEL_280cfa4d9a4c41c2b9d9fbc899b80290"
       ],
       "layout": "IPY_MODEL_b023b3499e19424db9a9ed33bbb4cd55"
      }
     },
     "fb2d2d4bf5b34a46aa1db18d9fe7f43f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1e8a8923d3b7448a894cde2ef3886965",
       "max": 5.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_96974e86b16c484c809d4bbaeec772e4",
       "value": 5.0
      }
     },
     "fc074c593c304e0ab08d1ca57af9f37f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fe6c51a4ef6a4577af91aaf74fcbd538": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_77fab8ebea724581980b206deec12aee",
       "placeholder": "​",
       "style": "IPY_MODEL_b4bfdf4f55f1489382e1d37df0ca484d",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "ff1566524f5d4c2ebd151841c1c2384d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
